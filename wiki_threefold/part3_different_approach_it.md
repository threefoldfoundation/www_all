## A different approach to creating and operating IT workloads

What if we can come up with a new concept for creating, deploying and operating IT workloads.  What if we say goodbye to the centralised way of controlling IT workloads.  Can we develop a concept that works in a decentralised way?

### The concept

To build this concept, we will draw use the human way of operating a large ship. The crew of a ship has different roles which we will call the *coordinators*. Some of these coordinators can be exampled as the captain, officers, engineers and oilers.  These coordinators have deep knowledge of their specific tasks but usually lack a detailed understanding of the other roles on the ship. In the "hive" of a ship, the specific requests to get tasks done are sent to coordinators who will receive, interpret, execute their own specific tasks and will report the (non-) progress of these tasks or subtasks.  As an example, the captain will create and agree to a schedule of maintenance tasks that need to be done in the engine room. These are daily, weekly and monthly tasks that should be completed by the engineers and oilers. The engineers and oilers do not get continuous input from the captain. They get one instruction to follow the schedule and know what needs to be done for each and every task. The intermediate state (by how much is a task complete and what still needs to be done) is stored by the coordinators executing the tasks. This is the most efficient way of accomplishing tasks.

### Jumping to the IT industry

If we bring this concept to the IT world we conclude that today’s IT systems are built in a significantly different way.  Most architectures (if not all) have a central facility that stores *all* information and *all* states of activity in a so-called central ‘database’. This is the general norm within IT. However these types of architectures are clearly inefficient and are very complex - such that as in the analogy of a ship -  a central command room (database) is created where every coordinator needs to report back* i) *the current level of activity and* ii*) completion (state) of each task and on top of that  every coordinator is required to have an understanding of the tasks to accomplish and those that are in process - hence leading to the outcome of unnecessary elements and complexity due to centralisation. 

In case of the ship example we deem this type of functioning/workflow to be inefficient however in the world of IT it is a common way.  To make matters worse, IT infrastructures usually have different databases to store "state" in for different parts of the IT architecture.  In our ship analogy would translate to different command rooms that orchestrate different functions on the ship. This, in turn, introduces the need for another role to start coordinating between the different command rooms to make sure that all information is shared and communicated. Taking this pattern forward leads to an endless loop of adding interfaces, layers, bridges between different parts of the system that all store information, multiple times and in different ways.  

### Datastores

By design, a decentralized IT  architecture should not use centralized data storage (databases) for multiple tasks (roles). It instead should feature a similar approach to the way of functioning as in the ‘ship’ example. In this architecture,  roles get short and precise instructions of the jobs to be completed and then autonomously execute on these instructions.  We call these execution engines *coordinators*.  Coordinators receive instructions, execute on instructions and store all information with regards to the current task *locally.*  They store all the relevant information in a local storage facility (available to the individual coordinators only) and provide information about the state of execution if and when other coordinators ask for a state update.